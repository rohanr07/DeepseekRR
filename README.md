# DeepSeekRR: Run Ollama's 32B Parameter Model Locally in VS Code

Welcome to **DeepSeekRR**, a Visual Studio Code extension that enables you to run Ollama's 32-billion parameter model directly within your VS Code environment. This extension facilitates seamless integration of advanced AI capabilities into your coding workflow.

## Features

- **Local Model Execution**: Leverage the power of Ollama's 32B parameter model without the need for external cloud services.
- **Seamless VS Code Integration**: Interact with the model directly from the VS Code interface, enhancing your development experience.
- **Customizable Settings**: Tailor the extension's behavior to suit your preferences through configurable options.

## Usage

Once the extension is active:
	â€¢	Running the Model:
	â€¢	Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).
	â€¢	Type DeepSeekRR: Run Model and press Enter.
	â€¢	The model will process the input and display the output within VS Code.

## Configuration

DeepSeekRR offers several settings to customize its behavior:
	â€¢	Model Path: Specify the local path to the Ollama 32B model.
	â€¢	Execution Parameters: Adjust parameters such as batch size and processing threads.

To modify these settings:
	1.	Go to File > Preferences > Settings.
	2.	Search for DeepSeekRR.
	3.	Update the settings as desired.

## Requirements

Before using DeepSeekRR, ensure you have the following:
	â€¢	Visual Studio Code: Version 1.60.0 or later.
	â€¢	Node.js: Version 14.x or later.
	â€¢	Ollama 32B Model: Ensure the model is downloaded and accessible on your local machine.

Note: For detailed information, please refer to the projectâ€™s source code and documentation, found here : https://api-docs.deepseek.com


Thanks ðŸ¤“
